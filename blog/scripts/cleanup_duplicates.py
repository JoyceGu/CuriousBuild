#!/usr/bin/env python3
"""
Notionæ•°æ®åº“é‡å¤æ–‡ç« æ¸…ç†è„šæœ¬
ç”¨äºè¯†åˆ«å’Œæ¸…ç†é‡å¤çš„æ–‡ç« 
"""

import os
import requests
import json
import re
from datetime import datetime
from pathlib import Path
from difflib import SequenceMatcher
from collections import defaultdict

def load_env_file():
    """Load environment variables from .env file"""
    env_path = Path(__file__).parent.parent.parent / '.env'
    if env_path.exists():
        with open(env_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

# Load environment variables from .env file
load_env_file()

class NotionDuplicateCleaner:
    def __init__(self):
        self.notion_token = os.getenv('NOTION_TOKEN')
        self.database_id = os.getenv('NOTION_DATABASE_ID')
        
        if not self.notion_token or not self.database_id:
            print("âŒ è¯·è®¾ç½®ç¯å¢ƒå˜é‡:")
            print("   export NOTION_TOKEN='your_notion_token'")
            print("   export NOTION_DATABASE_ID='your_database_id'")
            return
            
        self.headers = {
            'Authorization': f'Bearer {self.notion_token}',
            'Content-Type': 'application/json',
            'Notion-Version': '2022-06-28'
        }
    
    def query_all_posts(self):
        """æŸ¥è¯¢æ‰€æœ‰æ–‡ç« ï¼ˆåŒ…æ‹¬Publishedå’ŒDraftçŠ¶æ€ï¼‰"""
        url = f'https://api.notion.com/v1/databases/{self.database_id}/query'
        
        # æŸ¥è¯¢æ‰€æœ‰æ–‡ç« ï¼Œä¸ç­›é€‰çŠ¶æ€
        payload = {
            "sorts": [
                {
                    "property": "Date",
                    "direction": "descending"
                }
            ]
        }
        
        all_posts = []
        has_more = True
        start_cursor = None
        
        while has_more:
            if start_cursor:
                payload['start_cursor'] = start_cursor
            
            try:
                response = requests.post(url, headers=self.headers, json=payload)
                if response.status_code == 200:
                    data = response.json()
                    all_posts.extend(data['results'])
                    has_more = data.get('has_more', False)
                    start_cursor = data.get('next_cursor')
                else:
                    print(f"âŒ æŸ¥è¯¢Notionå¤±è´¥: {response.status_code}")
                    print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
                    break
            except Exception as e:
                print(f"âŒ è¿æ¥Notionå¤±è´¥: {e}")
                break
        
        print(f"ğŸ“š æ‰¾åˆ° {len(all_posts)} ç¯‡æ–‡ç« ï¼ˆæ‰€æœ‰çŠ¶æ€ï¼‰")
        return all_posts
    
    def get_page_content(self, page_id):
        """è·å–é¡µé¢å†…å®¹"""
        url = f'https://api.notion.com/v1/blocks/{page_id}/children'
        
        try:
            response = requests.get(url, headers=self.headers)
            if response.status_code == 200:
                return response.json()['results']
            else:
                return []
        except Exception as e:
            print(f"âŒ è·å–é¡µé¢å†…å®¹é”™è¯¯: {e}")
            return []
    
    def extract_rich_text(self, rich_text_array):
        """æå–å¯Œæ–‡æœ¬å†…å®¹"""
        if not rich_text_array:
            return ""
            
        result = []
        for text_obj in rich_text_array:
            text = text_obj.get('text', {}).get('content', '')
            result.append(text)
        
        return ''.join(result)
    
    def convert_notion_to_text(self, blocks):
        """å°†Notionå—è½¬æ¢ä¸ºçº¯æ–‡æœ¬ï¼ˆç”¨äºæ¯”è¾ƒï¼‰"""
        text_content = []
        
        for block in blocks:
            block_type = block.get('type')
            block_data = block.get(block_type, {})
            
            if block_type == 'paragraph':
                text = self.extract_rich_text(block_data.get('rich_text', []))
                if text.strip():
                    text_content.append(text)
            
            elif block_type in ['heading_1', 'heading_2', 'heading_3']:
                text = self.extract_rich_text(block_data.get('rich_text', []))
                if text.strip():
                    text_content.append(text)
            
            elif block_type in ['bulleted_list_item', 'numbered_list_item']:
                text = self.extract_rich_text(block_data.get('rich_text', []))
                if text.strip():
                    text_content.append(text)
            
            elif block_type == 'quote':
                text = self.extract_rich_text(block_data.get('rich_text', []))
                if text.strip():
                    text_content.append(text)
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬ï¼Œç§»é™¤å¤šä½™ç©ºæ ¼
        full_text = ' '.join(text_content)
        # ç§»é™¤æ‰€æœ‰æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼ï¼Œåªä¿ç•™å­—æ¯æ•°å­—ï¼Œç”¨äºæ¯”è¾ƒ
        clean_text = re.sub(r'[^\w]', '', full_text.lower())
        return clean_text
    
    def extract_page_properties(self, page):
        """æå–é¡µé¢å±æ€§"""
        properties = page.get('properties', {})
        
        # æ ‡é¢˜
        title = "Untitled"
        title_prop = properties.get('Title') or properties.get('Name')
        if title_prop and title_prop.get('title'):
            title = title_prop['title'][0]['text']['content']
        
        # çŠ¶æ€
        status = "Draft"
        status_prop = properties.get('Status')
        if status_prop and status_prop.get('select'):
            status = status_prop['select']['name']
        
        # æ—¥æœŸ
        date = datetime.now().strftime('%Y-%m-%d')
        date_prop = properties.get('Date')
        if date_prop and date_prop.get('date') and date_prop['date'].get('start'):
            date = date_prop['date']['start']
        
        return {
            'title': title,
            'status': status,
            'date': date,
            'page_id': page['id']
        }
    
    def calculate_similarity(self, text1, text2):
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
        if not text1 or not text2:
            return 0.0
        
        # ä½¿ç”¨SequenceMatcherè®¡ç®—ç›¸ä¼¼åº¦
        return SequenceMatcher(None, text1, text2).ratio()
    
    def find_duplicates(self):
        """æŸ¥æ‰¾é‡å¤çš„æ–‡ç« """
        print("ğŸ” å¼€å§‹æŸ¥æ‰¾é‡å¤æ–‡ç« ...")
        
        all_posts = self.query_all_posts()
        
        if len(all_posts) < 2:
            print("ğŸ“ æ–‡ç« æ•°é‡ä¸è¶³ï¼Œæ— æ³•æŸ¥æ‰¾é‡å¤")
            return []
        
        # æå–æ‰€æœ‰æ–‡ç« çš„ä¿¡æ¯
        posts_info = []
        for post in all_posts:
            try:
                properties = self.extract_page_properties(post)
                if properties['title'] == "Untitled":
                    continue
                
                # è·å–å†…å®¹
                blocks = self.get_page_content(post['id'])
                content_text = self.convert_notion_to_text(blocks)
                
                posts_info.append({
                    'page_id': post['id'],
                    'title': properties['title'],
                    'status': properties['status'],
                    'date': properties['date'],
                    'content': content_text,
                    'raw_post': post
                })
                
                print(f"  ğŸ“„ {properties['title']} ({properties['status']})")
            except Exception as e:
                print(f"  âš ï¸  å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {e}")
                continue
        
        # æŸ¥æ‰¾é‡å¤
        duplicates = []
        checked = set()
        
        for i, post1 in enumerate(posts_info):
            if post1['page_id'] in checked:
                continue
            
            similar_posts = [post1]
            
            for j, post2 in enumerate(posts_info[i+1:], start=i+1):
                if post2['page_id'] in checked:
                    continue
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = self.calculate_similarity(post1['content'], post2['content'])
                
                # å¦‚æœå†…å®¹ç›¸ä¼¼åº¦è¶…è¿‡70%ï¼Œæˆ–è€…å†…å®¹é•¿åº¦ç›¸ä¼¼ä¸”ç›¸ä¼¼åº¦è¶…è¿‡60%ï¼Œè®¤ä¸ºæ˜¯é‡å¤
                # åŒæ—¶æ£€æŸ¥æ ‡é¢˜æ˜¯å¦ç›¸ä¼¼ï¼ˆå¤„ç†æ”¹æ ‡é¢˜çš„æƒ…å†µï¼‰
                title_similarity = self.calculate_similarity(
                    re.sub(r'[^\w]', '', post1['title'].lower()),
                    re.sub(r'[^\w]', '', post2['title'].lower())
                )
                
                is_duplicate = False
                if similarity > 0.7:
                    is_duplicate = True
                elif similarity > 0.6 and abs(len(post1['content']) - len(post2['content'])) < max(len(post1['content']), len(post2['content'])) * 0.2:
                    # å†…å®¹é•¿åº¦ç›¸ä¼¼ä¸”ç›¸ä¼¼åº¦è¶…è¿‡60%
                    is_duplicate = True
                elif title_similarity > 0.5 and similarity > 0.5:
                    # æ ‡é¢˜ç›¸ä¼¼ä¸”å†…å®¹æœ‰ä¸€å®šç›¸ä¼¼åº¦
                    is_duplicate = True
                
                if is_duplicate:
                    print(f"    ğŸ” å‘ç°ç›¸ä¼¼: '{post1['title']}' vs '{post2['title']}' (ç›¸ä¼¼åº¦: {similarity:.2%}, æ ‡é¢˜ç›¸ä¼¼åº¦: {title_similarity:.2%})")
                    similar_posts.append(post2)
                    checked.add(post2['page_id'])
            
            if len(similar_posts) > 1:
                duplicates.append(similar_posts)
                checked.add(post1['page_id'])
        
        return duplicates
    
    def update_page_status(self, page_id, new_status="Draft"):
        """æ›´æ–°é¡µé¢çŠ¶æ€"""
        url = f'https://api.notion.com/v1/pages/{page_id}'
        
        payload = {
            "properties": {
                "Status": {
                    "select": {
                        "name": new_status
                    }
                }
            }
        }
        
        try:
            response = requests.patch(url, headers=self.headers, json=payload)
            if response.status_code == 200:
                return True
            else:
                print(f"  âŒ æ›´æ–°å¤±è´¥: {response.status_code} - {response.text}")
                return False
        except Exception as e:
            print(f"  âŒ æ›´æ–°é”™è¯¯: {e}")
            return False
    
    def cleanup_duplicates(self, auto_clean=False):
        """æ¸…ç†é‡å¤æ–‡ç« """
        duplicates = self.find_duplicates()
        
        if not duplicates:
            print("\nâœ… æ²¡æœ‰æ‰¾åˆ°é‡å¤æ–‡ç« ï¼")
            return
        
        print(f"\nğŸ” æ‰¾åˆ° {len(duplicates)} ç»„é‡å¤æ–‡ç« :")
        print("=" * 60)
        
        for idx, group in enumerate(duplicates, 1):
            print(f"\nğŸ“¦ é‡å¤ç»„ {idx} ({len(group)} ç¯‡æ–‡ç« ):")
            
            # æŒ‰æ—¥æœŸå’ŒçŠ¶æ€æ’åºï¼Œä¿ç•™æœ€æ–°çš„Publishedç‰ˆæœ¬
            group_sorted = sorted(group, key=lambda x: (
                x['status'] != 'Published',  # Publishedä¼˜å…ˆ
                x['date']  # æ—¥æœŸè¶Šæ–°è¶Šå¥½
            ), reverse=True)
            
            keep_post = group_sorted[0]
            duplicate_posts = group_sorted[1:]
            
            print(f"  âœ… ä¿ç•™: {keep_post['title']} ({keep_post['status']}, {keep_post['date']})")
            
            for dup in duplicate_posts:
                print(f"  ğŸ—‘ï¸  æ ‡è®°ä¸ºDraft: {dup['title']} ({dup['status']}, {dup['date']})")
            
            if auto_clean:
                print("\n  ğŸ”„ æ­£åœ¨æ›´æ–°çŠ¶æ€...")
                for dup in duplicate_posts:
                    if self.update_page_status(dup['page_id'], "Draft"):
                        print(f"  âœ… å·²å°† '{dup['title']}' æ ‡è®°ä¸ºDraft")
                    else:
                        print(f"  âŒ æ›´æ–° '{dup['title']}' å¤±è´¥")
            else:
                print("\n  ğŸ’¡ æç¤º: è¿è¡Œè„šæœ¬æ—¶æ·»åŠ  --auto å‚æ•°å°†è‡ªåŠ¨æ›´æ–°çŠ¶æ€")
        
        print("\n" + "=" * 60)
        print(f"ğŸ“Š æ€»ç»“: æ‰¾åˆ° {len(duplicates)} ç»„é‡å¤ï¼Œå…± {sum(len(g) - 1 for g in duplicates)} ç¯‡éœ€è¦å¤„ç†")

def main():
    import sys
    
    print("ğŸ§¹ Notionæ•°æ®åº“é‡å¤æ–‡ç« æ¸…ç†å·¥å…·")
    print("=" * 60)
    
    cleaner = NotionDuplicateCleaner()
    
    if not cleaner.notion_token or not cleaner.database_id:
        print("\nğŸ’¡ è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡åé‡æ–°è¿è¡Œ")
        return
    
    auto_clean = '--auto' in sys.argv or '-a' in sys.argv
    
    if auto_clean:
        print("âš ï¸  è‡ªåŠ¨æ¸…ç†æ¨¡å¼å·²å¯ç”¨ï¼Œå°†è‡ªåŠ¨å°†é‡å¤æ–‡ç« æ ‡è®°ä¸ºDraft")
        response = input("ç¡®è®¤ç»§ç»­? (yes/no): ")
        if response.lower() != 'yes':
            print("âŒ å·²å–æ¶ˆ")
            return
    
    cleaner.cleanup_duplicates(auto_clean=auto_clean)
    
    if auto_clean:
        print("\nâœ¨ æ¸…ç†å®Œæˆï¼")
        print("ğŸ’¡ å»ºè®®è¿è¡ŒåŒæ­¥è„šæœ¬é‡æ–°åŒæ­¥: cd blog && python3 sync_notion.py")

if __name__ == "__main__":
    main()

